# Fundamentos filosóficos de la Inteligencia Artificial

La inteligencia artificial es un tópico muy amplio cuyo planteo teórico y debate no son tema nuevo, ya que la inteligencia artificial existe como disciplina académica desde mediados del siglo pasado, pero cuyo debate empezó incluso algunos años antes. Más allá de las implicancias técnicas para crear IAs, lo que nos compete en esta ocasión es el planteo fundamental y filosófico de qué es una inteligencia artificial o más bien, de qué es capaz una inteligencia artificial, y las implicancias morales y consecuencias de su creación.

Todos los avances científicos en el área de la inteligencia artificial en la actualidad se paran sobre la base de que la inteligencia artificial es posible, pero para poder afirmar esto, evidentemente primero debe haber consenso acerca de qué define a una inteligencia artificial. En base a cómo planteemos esta definición, dependerá si la IA es teóricamente posible o es imposible. A lo largo de todos estos años de debate e incluso en la actualidad, coexisten dos hipótesis acerca de cuáles son las posibilidades y límites de las IAs. Estas dos hipótesis son la de la inteligencia artificial débil y Ia fuerte. Distintos científicos han presentado argumentos a favor y en contra de ambas hipótesis, pero en ningún caso han sido suficientes para aceptar completamente o descartar ninguna de las dos posibilidades. La hipótesis de la inteligencia artificial débil plantea que las IAs son capaces de actuar inteligentemente, es decir, simulan ser inteligentes. Mientras que la inteligencia artificial fuerte es la hipótesis que plantea que las IAs son realmente capaces de pensar y tienen una mente, no que simplemente simulan hacerlo. A continuación, se presentarán de forma resumida algunos de los numerosos argumentos a favor y en contra de cada hipótesis que han hecho distintos científicos a los largo de estos años.

## Inteligencia Artificial Débil

Como ya se dijo, la hipótesis de la inteligencia artificial débil plantea que las máquinas son capaces de simular que piensan, pero no lo hacen realmente. El científico Alan Turing propuso de hecho remplazar la pregunta de si las máquinas pueden pensar con un test de comportamiento para medir la inteligencia de las máquinas: el actualmente conocido Test de Turing.

A su vez, Turing también apoya el argumento de la incapacidad, es decir, afirma que hay cosas que las computadoras jamás podrán hacer. Con este argumento, lo que se quiere mostrar es que por más que ciertas computadoras muestren comportamientos que hacen parecer que piensan y entienden, en realidad es porque están programadas para ello y eso no significa que realmente piensen o entiendan, por lo cual son incapaces de realizar otras cosas que sí requieren de proceso mental y entendimiento.

Otro argumento a favor de esta hipótesis es el de la incompletitud. El filósofo J. R. Lucas afirma que el teorema de la incompletitud demuestra que las máquinas son mentalmente inferiores a los humanos, debido a que las primeras son sistemas que están limitados por este teorema, mientras que los humanos no lo están.

Por último, el argumento más influyente apoyando esta hipótesis es el de la informalidad. Planteado por Turing, afirma que hay comportamientos humanos demasiado complejos como para ser capturados en reglas (problema de la calificación) y como las máquinas sólo siguen conjuntos de reglas, entonces habrán comportamientos inteligentes humanos que nunca podrán generar, ya que no están calificadas para ello. El filósofo Dreyfus profundiza en este punto y destaca difentes problemas en la aquitectura de las redes neuronales, los cuales han sido mayoritariamente resueltos con el avance científico en el campo de la IA, demostrando su evolución y progreso. A su vez, este filósofo destaca que las IAs estarán en desventaja frente a los seres humanos siempre y cuando sigan sin experimentar la cognición desde dentro de un cuerpo, ya que los estímulos o conjunto de reglas a los que estará expuesta la IA aislada siempre será muchísimo más limitado en comparación a la experiencia sensorial de la que goza un humano corporeo.

## Inteligencia Artificial Fuerte

La hipótesis de la inteligencia artificial fuerte plantea que pueden haber máquinas que piensen. Pero para comenzar, debe haber concenso acerca de qué significa la palabra _pensar_ en este contexto. Hay distintas posturas al respecto: que la máquina debe ser **consciente** de su propio estado y acciones, que la máquina siente emociones o incluso que la maquina debe tener intenciones.

Turing afirma que no tiene caso preguntarse si las computadoras pueden pensar, ya que ni siquiera somos capaces de demostrar si otras personas piensan, sino que más bien tenemos una educada convención de asumir que las otras personas también piensan. Turing plantea que cuando llegue el punto en que dialogar con IAs esté normalizado, esta educada convención se extenderá inevitablemente a la IA, dejando atrás el debate y la distinción entre IA fuerte y débil.

Más allá de la educada convención de Turing, sigue siendo un hecho que podemos afirmar que los humanos tenemos mentes reales y que las máquinas no. Pero responder por qué nuestras mentes son reales (para luego responder por qué las demás no lo son) tampoco es tarea fácil. La corriente filosófica fisicalista afirma que cuerpo y mente no están separados y que los estados mentales son determinados por los estados cerebrales. A su vez, afirma que los estados mentales se pueden interpretar desde dos puntos de vista: el _contenido amplio_ (donde un observador omnisciente externo ve toda la situación durante el estado mental) y el _contenido reducido_ (que solo considera el estado cerebral durante dado estado mental). Y por último, que si queremos develar si una IA realmente piensa y tiene estados mentales, deberíamos concentrarnos en el contenido reducido, ya que es el estado cerebral el que define el próximo estado cerebral, no el contexto externo del sistema.

Por otro lado, la teoría del funcionalismo afirma que un estado mental es cualquier condición causal intermedia entre una entrada y una salida. Según esta teoría, dados dos sistemas cualesquiera con procesos causales isomórficos, estos tendrían el mismo estado mental. Es decir, una computadora sería capaz de tener el mismo estado mental que una persona. Dado un experimento hipotético en el que gradualmente se remplaza el cerebro de una persona por uno robótico, surge el interrogante de si el cerebro resultante conserva la consciencia. En el caso de afirmaramos que el cerebro no conservaría la consciencia, estaríamos apelando a que la consciencia es un suceso epifenomenal (que simplemente sucede). Pero en caso de que afirmaramos que el cerebro la conservaría, entonces también deberíamos aceptar que el cerebro conservaría la consciencia en caso de ser remplazado completo por cualquier circuíto que mapee entradas a salidas generando los mismos resultados que el cerebro original, lo cual puede parecer exagerado o descabellado.

Aunque la teoría del naturalismo biológico representa un gran desafío para el funcionalismo, ya que plantea que los estados mentales dependen de propiedades de las neuronas en sí mismas y no sólo de las entradas y salidas funcionales del sistema. Esta teoría plantea que el sistema debe _entender_ las entradas y salidas para ser considerado una mente, no solo usarlas y generarlas de la forma esperada sin comprenderlas. De igual modo, por más que refute al fisicalismo, esta teoría sigue resultanto una refutación de la IA fuerte más que un argumento a favor.

Como se puede notar, todos los argumentos sobre la IA fuerte giran en torno a la consciencia. Uno de los aspectos que componen a la consciencia es la experiencia subjetiva, como por qué se sienten de cierta forma los distintos estados mentales. Existe un término que denomina a la naturaleza intrínseca de las experiencias: _qualia_. Este último concepto también representa un desafío para el funcionalismo, ya que diferentes qualias pueden interferir en lo que en otro caso serían dos procesos causales isomóficos. Aunque el problema con el concepto de qualia es que por su naturaleza pone en jaque a toda la ciencia con una brecha explicativa. De modo que las posibilidades son o negar la existencia de la qualia o aceptar la brecha explivativa de que el ser humano es incapaz de comprender su propia existencia.

Es evidente, y ya lo adelantaba Turing, que no será posible resolver los dilemas filosóficos de las IAs hasta que no logremos desentrañarnos a nosotros mismos y nuestra consciencia, pero de todos modos nos es posible continuar avanzando en el campo de la IA y creando sistemas que sean funcionales, sin preocuparnos aún por el hecho de que tengan consciencia.

## Ética y Riesgos de Desarrollar Inteligencia Artificial

En el área de la inteligencia artificial no son pocos los debates y opiniones acerca de las posibilidades de las IAs en sí mismas. Esta pregunta sobre las posibilidades refiere a si, en el caso hipotético de que los humanos _fueran capaces_ de construir IAs que cumplieran con la definición teórica consensuada de Inteligencia Artificial, ¿_deberían_ los humanos desarrollar IAs? Más allá del desacuerdo filosófico en la definición de IA, otro gran factor de debate desde el origen del concepto ha sido si sería ético y cuáles serían las posibles consecuencias del hecho de construir Inteligencia Artificial. A medida que más avanza la investigación científica en este area, más polémico y urgente se torna el debate en torno a la ética de la IA, ya que su desarrollo podría ser un factor que defina de forma negativa en gran medida el futuro de la humanidad (como podría también no afectarlo notoriamente o hacerlo positivamente, aunque nos parezca menos probable la posibilidad). No por nada la Inteligencia Artificial ha sido un gran inspirador de numerosas novelas y películas de ciencia ficción y futuros distópicos. Entre todos los posibles argumentos en contra del desarrollo de la Inteligencia Artificial, destacaremos los seis siguientes como los más tratados y a su vez se explicará mínimamente el debate en torno a cada uno.

1. Muchas personas podrían perder sus trabajos debido a la automatización. Es indiscutible que ya existen (y seguirán creándose) robots e inteligencias artificiales capaces de remplazar a las personas en el desarrollo de trabajos específicos. Pero también se debe tener en cuenta que la implementación de inteligencias artificiales en ciertas áreas (como en finanzas) ha abierto posibilidades y nichos previamente inexistentes, generando distintos trabajos para personas calificadas en puestos antes inexistentes.

2. Las personas podrían tener mucho o muy poco tiempo de ocio. Si bien las predicciones al respecto dictaban que con el avance de la IA las personas tendrían cada vez más tiempo de ocio, con el avance actual en el campo, se ha revelado que para muchas personas la situación parece ser al contrario. Esto es debido a la implementación de inteligencia artificial en distintos nichos de las industrias, la competencia debe invertir gradualmente más y más tiempo para mantenerse innovando y al día para poder igualar o superar a las otras empresas del rubro. De modo que las personas involucradas se presionan cada vez más y más, invirtiendo más tiempo de trabajo para mantenerse compitiendo.

3. Las personas podrían perder su sentido de unicidad. La humanidad tiene un fuerte sentido de unicidad (de sentirse "únicos" o "especiales") por el simple hecho de su existencia como unica especie pensante conocida. El desarrollo de la IA podría amenazar esta _sensación de ser especiales_ de las personas, debido a las capacidades de la IA equiparables o superiores a las de los humanos. En contra de esta opinión, hay como evidencia la resiliencia previa de la humanidad en situaciones simililares a lo largo de la historia, como cuando se descubrió que la Tierra no era el centro del sistema solar o cuando Darwin posicionó al Homo Sapiens al mismo nivel que todas las demás especies.

4. La IA podría usarse para fines indeseados. Si bien es innegable que la IA podría utilizarse para numerosos fines destructivos (de hecho ya está en uso actualmente en aplicaciones militares), la realidad es que no sería la primera tecnología avanzada que se usara para tales fines. Más aún, es una frase icónica y que parece completamente real hoy en día la siguiente: "Se dice que una ciencia es útil si tiende a acentuar las desigualdades existentes [...] o promueve directamente la destrucción de vidas humanas".

5. El uso de sistemas de IA puede conducir a la pérdida de responsabilidad. Este es un dilema se plantea para el caso en que ocurra un problema en un sistema a causa de acciones de una IA o en las que intervino una IA, puede ser complicado o imposible determinar quién es el responsable: si es la IA, los humanos que participaron en el desarrollo de esa IA o los humanos que participan en el sistema.

6. El éxito de la IA puede significar el fin de la raza humana. Este último planteo es el más extremo pero interesante de todos y su debate es sumamente amplio. "Casi cualquier tecnología tiene el poder para hacer daño en las manos equivocadas, pero con la IA y robótica, surge el nuevo problema de que las manos equivocadas podrían pertenecer a la tecnología en sí misma". Esta frase es suficiente explicación para el planteo y a su vez ha sido el disparador de muchas novelas, pero la pregunta que deberíamos hacernos para plantear correctamente nuestros argumentos es _¿por qué la IA representa mayor riesgo que el software tradicional?_ Entre varios, podemos nombrar de forma resumida tres de los principales posibles riesgos de la IA:
7. Que un sistema de IA tenga estimaciones incorrectas, lo que lo haga tomar decisiones incorrectas (que generen daño).
8. El diseño de la función de utilidad para maximizar la eficacia de un sistema de IA podría generar comportamientos indeseados o decisiones extremas. Dado que es complejo diseñar estas funciones, es un escenario muy posible. Un ejemplo distópico sería que una IA diseñada para evitar el sufrimiento humano decidiera acabar con la humanidad para ahorrarles el sufrimiento (poco probable si la IA es suficientemente inteligente).
9. El diseño de la función de aprendizaje de un sistema de IA podría hacer que evolucione en un sistema con comportamientos indeseados. Por ejemplo, que los sistemas evolucionaran de tal forma que empezaran a construír ellos mismos sistemas cada vez más complejos y dejaran atrás a la raza humana. Una contrapropuesta para este punto es la posibilidad de desarrollar _IAs amigables_, es decir diseñar IAs desde el comienzo con funciones de balance y restricciones de modo que a pesar de que evolucionen, siempre se mantengan "amigables" con los seres humanos, es decir, que en ningún momento puedan desarrollar intenciones de dañar a los humanos. En el mundo de la ciencia ficción, por ejemplo, Isaac Asimov plantea las tres leyes de la robótica para lograr este cometido en su mundo de ficción.

Por último, resulta interesante darle lugar también a un último debate actual: si se llega al punto en que las IAs tengan tal nivel de inteligencia que las situaciones previamente planteadas fueran posibles, ¿no tendrían acaso también la capacidad de pensar que es injusto que los humanos les inpongamos restricciones para garantizar nuestra propia seguridad, pero no les garanticemos su propia seguridad y existencia a través de ningún derecho? Si bien parece una pregunta sacada completamente de películas de ciencia ficción, no deberíamos descartar la posibilidad de que sea un dilema con el que como humanidad nos tengamos que enfrentar de cara en un futuro no tan lejano.

## Resumen

A continuación, se presenta un mapa mental a modo de resumen de los temas expuestos hasta el momento, antes de proceder con la conclusión personal acerca de todos estos temas.

(Mapa mental disponible en la misma carpeta)

## Opinión Personal

En mi postura personal y luego de haberme encontrado por primera vez con argumentos tan diversos en estos debates éticos y filosóficos acerca de la IA, puedo afirmar que concuerdo con Turing cuando dice que no seremos capaces de resolver los dilemas en torno a la IA fuerte y débil si no logramos primero entender nuestra propia consciencia.

La sensación que me queda es que la IA débil y fuerte son dos caras de una misma moneda, dos pasos consecutivos en la evolución de la IA. Creo que la única definición que somos capaces de alcanzar con el avance tecnológico hoy en día es la de IA débil, o sea, somos capaces de simular inteligencia. Pero que en el momento en que logremos desentrañar los asuntos concernientes a la consciencia y la mente humana, vamos a estar posibilitados de dar el paso a la IA fuerte, y ser finalmente capaces de generar inteligencia artificialmente.

A su vez, me sorprende la cantidad de veces que salió a flote el tema de la ciencia ficción, que uno disfruta desde la inocencia de la _ficción_ pero sin ser realmente consciente de la _ciencia_ detrás de cada historia. Honestamente espero que la humanidad pueda en el futuro cercano continuar avanzando en el ámbito de la IA (y estoy segura de que lo hará) pero logrando mantener un equilibrio y aprendiendo a convivir pacíficamente con la IA en la vida cotidiana. Lamentablemente dudo lo último, ya que creo que si las manos equivocadas no son las de la IA, entonces algún humano se encargará de que sus propias manos sean las equivocadas y le dé un mal uso a esta tecnología. Pienso que así como existe el debate acerca de si los humanos son inherentemente buenos o inherentemente malos, el mismo debate se revelará con las IAs cuando lleguen al punto de tener consciencia. Aunque la respuesta a ese dilema solo la podríamos obtener en caso de que las IAs fueran inherentemente buenas, ya que caso contrario probablemente la raza humana desaparecería antes de que la pregunta se respondiera sola, ¿no?
